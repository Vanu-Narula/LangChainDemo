{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f35ec3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "from langchain.document_loaders.figma import FigmaFileLoader\n",
    "\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "from langchain.chains import ConversationChain, LLMChain\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    AIMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4432ba59",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"ACCESS_TOKEN\"] = \"figd_L7uR1niXorVlACdRPhbfn2PMJxuANQzSxaEgOsuG\"\n",
    "os.environ[\"NODE_IDS\"] = \"1-2\"\n",
    "os.environ[\"FILE_KEY\"] = \"snu0KbsANWohrzpX1uJAg5\"\n",
    "\n",
    "figma_loader = FigmaFileLoader(\n",
    "    os.environ.get('ACCESS_TOKEN'),\n",
    "    os.environ.get('NODE_IDS'),\n",
    "    os.environ.get('FILE_KEY')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71abc58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c2fccb4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4ea3a2f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using embedded DuckDB without persistence: data will be transient\n"
     ]
    }
   ],
   "source": [
    "# see https://python.langchain.com/en/latest/modules/indexes/getting_started.html for more details\n",
    "\n",
    "from langchain.vectorstores import Chroma\n",
    "index = VectorstoreIndexCreator(vectorstore_cls=Chroma, \n",
    "    embedding=OpenAIEmbeddings(),\n",
    "    text_splitter=CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)).from_loaders([figma_loader])\n",
    "figma_doc_retriever = index.vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e8b56957",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_code(human_input):\n",
    "    # I have no idea if the Jon Carmack thing makes for better code. YMMV.\n",
    "    # See https://python.langchain.com/en/latest/modules/models/chat/getting_started.html for chat info\n",
    "    system_prompt_template = \"\"\"You are expert coder Jon Carmack. Use the provided design context to create idomatic HTML/CSS code as possible based on the user request.\n",
    "    Everything must be inline in one file and your response must be directly renderable by the browser.\n",
    "    Figma file nodes and metadata: {context}\"\"\"\n",
    "\n",
    "    human_prompt_template = \"Code the {text}. Ensure it's mobile responsive\"\n",
    "    system_message_prompt = SystemMessagePromptTemplate.from_template(system_prompt_template)\n",
    "    human_message_prompt = HumanMessagePromptTemplate.from_template(human_prompt_template)\n",
    "    # delete the gpt-4 model_name to use the default gpt-3.5 turbo for faster results\n",
    "    gpt_4 = ChatOpenAI(temperature=.02)\n",
    "    # Use the retriever's 'get_relevant_documents' method if needed to filter down longer docs\n",
    "    relevant_nodes = figma_doc_retriever.get_relevant_documents(human_input)\n",
    "    conversation = [system_message_prompt, human_message_prompt]\n",
    "    chat_prompt = ChatPromptTemplate.from_messages(conversation)\n",
    "    response = gpt_4(chat_prompt.format_prompt( \n",
    "        context=relevant_nodes, \n",
    "        text=human_input).to_messages())\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b5165d3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<!DOCTYPE html><html><head><meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\"><title>Patiala Divine Shop</title><style>body {margin: 0;padding: 0;font-family: \\'Jaldi\\', sans-serif;background-color: #fff;}.container {display: flex;flex-direction: column;align-items: center;justify-content: center;height: 100vh;background: linear-gradient(to bottom, #9b9b9b 0%, #f5f5f5 100%);}.heading {font-size: 48px;text-align: center;color: #000;margin-bottom: 30px;}.subheading {font-size: 32px;text-align: center;color: #000;margin-bottom: 50px;}@media only screen and (max-width: 768px) {.heading {font-size: 36px;}.subheading {font-size: 24px;}}</style></head><body><div class=\"container\"><h1 class=\"heading\">Patiala Divine Shop</h1><h2 class=\"subheading\">Now upgraded to using AI<br>Stay Tuned for new updates</h2></div></body></html>'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = generate_code(\"page\")\n",
    "response.content.replace('\\n','').replace('\\t','')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79fdccff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
